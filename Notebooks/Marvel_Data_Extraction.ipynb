{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import whisper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean the extracted text from the Wikipedia page\n",
    "    text: The text to clean\n",
    "    \"\"\"\n",
    "    # Here we want to remove any text within square brackets, as it's often used for annotations\n",
    "    text = re.sub(r'\\[\\w\\]', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Here we return the text with leading and trailing whitespaces removed\n",
    "    return text\n",
    "\n",
    "def wiki_summary(url, output_filename):\n",
    "    # Here we use the requests library to retrieve the webpage\n",
    "    response = requests.get(url)\n",
    "    # If the connection was made, then we proceed to extract the content\n",
    "    if response.status_code == 200:\n",
    "        # We define the parser, which is the tool used to extract the content\n",
    "        # We use BeautifulSoup to parse the webpage content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # The plot content is usually within the 'mw-parser-output' div tag. We direct the soap object to find this tag\n",
    "        content_wrapper = soup.find('div', class_='mw-parser-output')\n",
    "        # Here we create a placeholder for the extracted text\n",
    "        summary_text = ''\n",
    "        # If we find the content wrapper, then we proceed to extract the plot\n",
    "        if content_wrapper:\n",
    "            # The plot is usually under the 'span' tag with the id 'Plot' or 'Episodes'\n",
    "            plot_heading = content_wrapper.find('span', id='Episodes')\n",
    "            # If we find either the plot or episodes heading, then we proceed to extract the text\n",
    "            if plot_heading:\n",
    "                # We iterate through each element after the plot heading until we find the next section heading\n",
    "                for elem in plot_heading.parent.find_next_siblings():\n",
    "                    # If we find another section heading, then we stop\n",
    "                    if elem.name in ['h2', 'h3']: \n",
    "                        break\n",
    "                    # Here we append the text to the placeholder summary_text\n",
    "                    summary_text += elem.get_text(separator=\"\\n\", strip=True) + '\\n\\n'\n",
    "        \n",
    "        # We can call the clean_text function to clean the extracted text\n",
    "        summary_text = clean_text(summary_text)\n",
    "        \n",
    "        # Finally, we create a file and write the summary to the file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary_text)\n",
    "\n",
    "        print(f\"Summary has been successfully saved to '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"Error. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary has been successfully saved to '/Users/sveerisetti/Desktop/Duke_Spring/LLM/Assignments/Assignment2/Scripts/Marvels.txt'\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the wiki_summary function\n",
    "wiki_summary('https://en.wikipedia.org/wiki/The_Marvels', '/Users/sveerisetti/Desktop/Duke_Spring/LLM/Assignments/Assignment2/Scripts/Marvels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from Marvel Cinematic Universe Fandom Wiki Page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content under classes ['mw-parser-output'] has been saved to '/Users/sveerisetti/Desktop/Duke_Spring/LLM/Assignments/Assignment2/Marvel/Lore2/Avengers_Echo.txt'\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Purpose: Clean the extracted text from the Wikipedia page\n",
    "    text: The text to clean\n",
    "    \"\"\"\n",
    "    # Here we want to remove any text within square brackets, as it's often used for annotations\n",
    "    text = re.sub(r'\\[\\w\\]', '', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    # Here we return the text with leading and trailing whitespaces removed\n",
    "    return text\n",
    "\n",
    "def extract_sections(url, output_filename, class_names):\n",
    "    try:\n",
    "        # Here we use the requests library to retrieve the webpage\n",
    "        response = requests.get(url)\n",
    "        # Raise an exception if the status code is not 200\n",
    "        response.raise_for_status() \n",
    "\n",
    "        # Define the parser, which is the tool used to extract the content. In this case we use BeautifulSoup to parse the webpage content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Placeholder for the extracted text\n",
    "        summary_text = ''\n",
    "\n",
    "        # Iterate through each class name and extract the content\n",
    "        for class_name in class_names:\n",
    "            # Find the content under the class name\n",
    "            content = soup.find('div', class_=class_name)\n",
    "            # If we find the content, then we append it to the summary_text\n",
    "            if content:\n",
    "                # We can then append the text to the placeholder summary_text\n",
    "                summary_text += content.get_text(separator=\"\\n\", strip=True) + '\\n\\n'\n",
    "        \n",
    "        # We can use the clean_text function to clean the extracted text\n",
    "        summary_text = clean_text(summary_text)\n",
    "\n",
    "        # Finally, we create a file and write the summary to the file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary_text)\n",
    "\n",
    "        # Print a success message\n",
    "        print(f\"Content under classes {class_names} has been successfully saved to '{output_filename}'\")\n",
    "    except requests.HTTPError as e:\n",
    "        # If an HTTP error occurs, then we print the error message\n",
    "        print(f\"Failed to retrieve the webpage. HTTP Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the extract_sections function\n",
    "extract_sections('https://marvelcinematicuniverse.fandom.com/wiki/Echo', '/Users/sveerisetti/Desktop/Duke_Spring/LLM/Assignments/Assignment2/Marvel/Lore2/Avengers_Echo.txt', ['mw-parser-output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, pipeline\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "import certifi\n",
    "import fitz\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tokenizer, Model, and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we utilize the Bert tokenizer in order to tokenize the input text\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Here we utilize the Sentence Transformer model to understand the semantic meaning of the input text. This will come in \n",
    "# handy when we want to find the most similar sentence to a given input sentence\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# This is the connection to the MongoDB database on its Atlas platform that we are using to store the data\n",
    "uri = \"mongodb+srv://sveerisetti:HRsAs1@assignment1cluster.lkuwikx.mongodb.net/?retryWrites=true&w=majority\"\n",
    "ca = certifi.where()\n",
    "# Here we create the client that will be used to connect to the MongoDB database\n",
    "client = MongoClient(uri, tlsCAFile=ca)\n",
    "# Here we select the database and the collection that we want to use\n",
    "db = client['Marvel']\n",
    "collection = db['superhero_chunk']\n",
    "# This is the key for the OpenAI API that we will use to generate the responses to the user's questions\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will take in a string and return the embeddings for the string. \n",
    "# Here we create a vector of embeddings for the input text\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Extract Text and Store Chunks in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_store_with_embedding(directory_path):\n",
    "    \"\"\"\n",
    "    Purpose: To extract text from .txt files in a directory and store the text along with its embedding in MongoDB.\n",
    "    directory_path: The path to the directory containing .txt files.\n",
    "    \"\"\"\n",
    "    # Here we divide the text into chunks of 100 words\n",
    "    def chunk_words(words, chunk_size=100):\n",
    "        # Here we iterate through the words in the text and create chunks of 100 words\n",
    "        for i in range(0, len(words), chunk_size): \n",
    "            yield ' '.join(words[i:i+chunk_size])\n",
    "\n",
    "    # Here we iterate over the words in the text and create chunks of 100 words. \n",
    "    # The main point is to direct the function to the directory where the text files are stored\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):  \n",
    "            # Here we only focus on the .txt files in the directory\n",
    "            txt_path = os.path.join(directory_path, filename)\n",
    "            # We open the text file and read the text\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                words = text.split()\n",
    "                # Here we iterate over the chunks of 100 words and store the chunks in the MongoDB database\n",
    "                for chunk in chunk_words(words):\n",
    "                    # The chunk embedding is generated using the Sentence Transformer model\n",
    "                    chunk_embedding = generate_embedding(chunk).tolist()  \n",
    "                    # Here we store the chunk and its embedding in the MongoDB database\n",
    "                    collection.insert_one({\n",
    "                        \"chunk\": chunk,\n",
    "                        \"embedding\": chunk_embedding,\n",
    "                        \"source\": os.path.basename(txt_path)\n",
    "                    })\n",
    "            print(f\"Successfully stored chunks from: '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and stored chunks from 'Avenger_Strange.txt'\n",
      "Processed and stored chunks from 'AntMan_Quantumania.txt'\n",
      "Processed and stored chunks from 'wakanda_forever.txt'\n",
      "Processed and stored chunks from 'Echo.txt'\n",
      "Processed and stored chunks from 'love_and_thunder.txt'\n",
      "Processed and stored chunks from 'Avenger_Loki.txt'\n",
      "Processed and stored chunks from 'doctor_strange_summary.txt'\n",
      "Processed and stored chunks from 'Avenger_Thor.txt'\n",
      "Processed and stored chunks from 'Avengers_Echo.txt'\n",
      "Processed and stored chunks from 'Avenger_GOG3.txt'\n",
      "Processed and stored chunks from 'Avengers_Marvels.txt'\n",
      "Processed and stored chunks from 'Avenger_Antman.txt'\n",
      "Processed and stored chunks from 'Avenger_BlackPanther.txt'\n",
      "Processed and stored chunks from 'Loki.txt'\n",
      "Processed and stored chunks from 'GOG_3.txt'\n",
      "Processed and stored chunks from 'Marvels.txt'\n"
     ]
    }
   ],
   "source": [
    "# Sample Execution\n",
    "# Set the directory path to the directory containing the .txt files\n",
    "directory_path = \"/Users/sveerisetti/Desktop/Duke_Spring/LLM/Assignments/Assignment2/Marvel/Lore2\"  \n",
    "extract_text_and_store_with_embedding(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Most Relevant Chunks for a Given Query Based on Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Purpose: Here we want to find the most relevant chunks from the MongoDB database to a given query.\n",
    "    query: The input query for which we want to find the most relevant chunks.\n",
    "    top_k: The number of most relevant chunks to return.\n",
    "    \"\"\"\n",
    "    # Here we genearte the embedding for the query using the Sentence Transformer model and the generate_embedding function. \n",
    "    query_embedding = generate_embedding(query)\n",
    "    docs = collection.find({})\n",
    "\n",
    "    # Here we gather all of the document embeddings and store them in a list. These are the similarities between the query and the chunks\n",
    "    # in the MongoDB database. \n",
    "    similarities = []\n",
    "    for doc in docs:\n",
    "        # To perform the cosine similarity function we need to make sure that the embeddings are in the correct format. \n",
    "        # To do this, we convert the embeddings to a list of floats.\n",
    "        chunk_embedding = [float(value) for value in doc['embedding']]\n",
    "        query_embedding = [float(value) for value in query_embedding]\n",
    "        # Here we calculate the cosine similarity between the query and the chunk in the MongoDB database\n",
    "        similarity = 1 - cosine(chunk_embedding, query_embedding)\n",
    "        # We then append the chunk, similarity, and source to the similarities list\n",
    "        similarities.append((doc['chunk'], similarity, doc.get('source')))\n",
    "\n",
    "    # Here sort the similarities list by the similarity score in descending order. \n",
    "    # The top will be the most similar chunks to the input query.\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    seen_chunks = set()\n",
    "    unique_similarities = []\n",
    "    # Here we loop through the similarities list and append the unique chunks to the unique_similarities list\n",
    "    for chunk, similarity, source in similarities:\n",
    "        # Checks for duplicate chunks\n",
    "        if chunk not in seen_chunks:\n",
    "            # Adds the chunk to the seen_chunks set\n",
    "            seen_chunks.add(chunk)\n",
    "            # Appends the unique chunk to the unique_similarities list\n",
    "            unique_similarities.append((chunk, similarity, source))\n",
    "            # Makes sure that the number of returned chunks is equal to the top_k\n",
    "            if len(unique_similarities) == top_k:\n",
    "                break\n",
    "    return unique_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Enhanced Prompt with the Most Relevant Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_with_context(relevant_chunks, query):\n",
    "    \"\"\"\n",
    "    Purpose: Generates a prompt with the relevant chunks and the input query.\n",
    "    relevant_chunks: The most relevant chunks to the input query.\n",
    "    query: The input query.\n",
    "    \"\"\"\n",
    "    # Here we build context for the prompt by adding the relevant chunks to the prompt\n",
    "    context = \"Based on the following information: \"\n",
    "    # Here we loop through the relevant chunks and add them to the context\n",
    "    for chunk, similarity, source in relevant_chunks:\n",
    "        # Here we add the source and similarity of the chunk to the context for the user to see \n",
    "        context += f\"\\n- [Source: {source}, Similarity: {similarity}]: {chunk}\"\n",
    "    # Here we concatenate the context and the input query to create the prompt to make a more efficient response\n",
    "    prompt = f\"{context}\\n\\n{query}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_text_with_gpt35(prompt, max_tokens=3100):\n",
    "    \"\"\"\n",
    "    Purpose: Generates a response to the user's query using the GPT-3.5 model.\n",
    "    prompt: The prompt to generate the response.\n",
    "    max_tokens: The maximum number of tokens for the response.\n",
    "    \"\"\"\n",
    "    # Here we generate the response to the user's query using the GPT-3.5 model\n",
    "    # We use the openai.ChatCompletion.create function to generate the response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        # Define the model of choice\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        # Define the maximum number of tokens for the response\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant chunks used to supplement the language model:\n",
      "Chunk: Martin and Katharyn Blair Justin Benson & Aaron Moorhead The TVA 's Loom nears catastrophic failure but Loki , Mobius , and Sylvie have a He Who Remains variant . Science/Fiction 2.05 November 2, 2023 Eric Martin Justin Benson & Aaron Moorhead Loki traverses dying timelines in an attempt to find his friends, but Reality is not what it seems. Glorious Purpose 2.06 November 9, 2023 Eric Martin Justin Benson & Aaron Moorhead Loki learns the nature of \"glorious purpose\" as he rectifies the past in this gripping finale. Videos [ ] Trailers [ ] Marvel Studios’ Loki Season 2\n",
      "Similarity: 0.5311649889887929\n",
      "Source: Avenger_Loki.txt\n",
      "\n",
      "Chunk: onto a Time Variance Authority Mail Cart and into the monitor room where Casey rats him out. Suddenly, Loki's body becomes distorted as he jumps to another point in the TVA where Casey does recognize him. This keeps happening to Loki as he comes to find out he is jumping from the past and present. In the past, He Who Remains made himself known to agents instead of replacing himself with the Time-Keepers . When Loki finally gets to Mobius, they go to see Ouroboros , a TVA repairs and advancement member who explains that Loki is Time Slipping .\n",
      "Similarity: 0.5235427891599219\n",
      "Source: Avenger_Loki.txt\n",
      "\n",
      "Chunk: Loki is pruned As Ouroboros explains, Loki slips in an dout of the present, eventually talking to past Ouroboros and creating the Temporal Aura Extractor . Ouroboros then explains that Loki had to prune himself in order to rip himself from time at the same moment that Mobius shot the extractor at the Temporal Loom , which is a device that turns raw time into energy. Right when they were about to do this, Loki slipped into the future. As he searched for a Time Stick , he realized it was too late. He eventually wandered to an elevator where\n",
      "Similarity: 0.5119599957372869\n",
      "Source: Avenger_Loki.txt\n",
      "\n",
      "Chunk: Loki Season Two Episodes 6 Original Channel Disney+ Original Run October 5, 2023 - November 9, 2023 Marvel Cinematic Universe ← Previous Season One \"I know what kind of god I need to be. For you. For all of us.\" ― Loki to Mobius M. Mobius and Sylvie Laufeydottir [src] The Second Season of Loki premiered on Disney+ on October 5, 2023 and concluded on November 9, 2023. Contents 1 Synopsis 2 Plot 3 Cast 3.1 Starring Cast 4 Episodes 5 Videos 5.1 Trailers 5.2 TV Spots 5.3 Clips 5.4 Featurettes 5.5 Other 6 References Synopsis [ ] Loki Season\n",
      "Similarity: 0.5086340789430133\n",
      "Source: Avenger_Loki.txt\n",
      "\n",
      "Chunk: No. overall No. in season Title Directed by Written by Original release date 7 1 \" Ouroboros \" Justin Benson & Aaron Moorhead Eric Martin October 5, 2023 ( 2023-10-05 ) In the past, the Time Variance Authority (TVA) attempts to apprehend Loki while he is uncontrollably warping across time in their headquarters. In the present, Loki reunites with Mobius M. Mobius and warns him of the threat posed by the many variants of He Who Remains , the TVA's creator. Concurrently, TVA General Dox has several TVA hunters arm themselves, ostensibly to find Sylvie , who caused the Sacred\n",
      "Similarity: 0.5049801880818879\n",
      "Source: Loki.txt\n",
      "\n",
      "Generated Text: In Season 2 of Loki, Ouroboros played a role in helping Loki. According to the information provided, Ouroboros explains to Loki that he is time slipping, constantly jumping from the past to the present. Ouroboros also reveals the existence of the Temporal Aura Extractor, which Loki uses to rip himself from time. Additionally, Ouroboros mentions the Temporal Loom, a device that turns raw time into energy. It seems that Ouroboros provides crucial information and assistance to Loki in his journey throughout the season.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The main function that is used to execute the code\n",
    "if __name__ == \"__main__\":\n",
    "    # The query that the user wants to ask\n",
    "    query = \"How did Ouroboros help in Season 2 of Loki?\"\n",
    "    # Call the function to find the most relevant chunks in the MongoDB database to the input query\n",
    "    relevant_chunks = find_most_relevant_chunks(query)\n",
    "    # If any relevant chunks were found, we will use these chunks to supplement the language model\n",
    "    if relevant_chunks:\n",
    "        print(\"Relevant chunks used to supplement the language model:\")\n",
    "        # Here we loop through the relevant chunks and print them out to the user. \n",
    "        # Then we give them the similarity and source of the chunk\n",
    "        for chunk, similarity, source in relevant_chunks:\n",
    "            print(f\"Chunk: {chunk}\\nSimilarity: {similarity}\\nSource: {source}\\n\")\n",
    "        \n",
    "        # Here we generate the prompt with the relevant chunks and the input query\n",
    "        prompt = generate_prompt_with_context(relevant_chunks, query)\n",
    "        # Here we use the GPT-3.5 model to generate the response to the user's query\n",
    "        generated_text = generate_text_with_gpt35(prompt)\n",
    "        # Here we print the generated text to the user\n",
    "        print(f\"Generated Text: {generated_text}\\n\")\n",
    "    else:\n",
    "        # Alternatively, if no relevant chunks were found, we will print a message to the user\n",
    "        print(\"No relevant chunks found in the database for the query.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
